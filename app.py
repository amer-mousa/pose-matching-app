import streamlit as st
import cv2
import mediapipe as mp
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import tempfile

st.set_page_config(page_title="ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª", layout="centered")
st.title("ğŸ¤– Ù…Ø·Ø§Ø¨Ù‚Ø© Ø­Ø±ÙƒØ© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ")

def extract_landmarks(video_path):
    mp_pose = mp.solutions.pose
    pose = mp_pose.Pose(static_image_mode=False)
    cap = cv2.VideoCapture(video_path)
    results = []

    while cap.isOpened():
        success, frame = cap.read()
        if not success:
            break
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        res = pose.process(rgb)
        if res.pose_landmarks:
            landmarks = [(lm.x, lm.y, lm.z) for lm in res.pose_landmarks.landmark]
            results.append(landmarks)
    cap.release()
    return results

def compare_landmarks(ref, live):
    n = min(len(ref), len(live))
    sims = []
    for i in range(n):
        a = np.array(ref[i]).flatten().reshape(1, -1)
        b = np.array(live[i]).flatten().reshape(1, -1)
        sim = cosine_similarity(a, b)[0][0]
        sims.append(sim)
    return np.mean(sims)

ref_video = st.file_uploader("ğŸ“¥ Ø­Ù…Ù‘Ù„ ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ù…Ù‡Ø§Ø±Ø© Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©", type=["mp4"])
live_video = st.file_uploader("ğŸ“· Ø­Ù…Ù‘Ù„ ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©", type=["mp4"])

if ref_video and live_video:
    st.info("â³ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©ØŒ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±...")

    # Ø­ÙØ¸ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ù…Ø¤Ù‚ØªØ§Ù‹
    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as f1:
        f1.write(ref_video.read())
        ref_path = f1.name
    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as f2:
        f2.write(live_video.read())
        live_path = f2.name

    ref_lm = extract_landmarks(ref_path)
    live_lm = extract_landmarks(live_path)

    score = compare_landmarks(ref_lm, live_lm)
    st.success(f"âœ… Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚: {score * 10:.2f} Ù…Ù† 10")
